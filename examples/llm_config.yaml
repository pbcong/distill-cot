mode: logit # Can be 'logit' or 'language'

model:
  teacher:
    name: "deepseek-coder-33b"
    path: "deepseek-ai/deepseek-coder-33b-base"
    device: "cuda"
    dtype: "bfloat16"
  student: "deepseek-ai/deepseek-coder-1.3b-base"

distillation:
  temperature: 2.0 # Higher temperature produces softer probability distributions
  alpha: 0.7 # Weight between distillation and task loss

training:
  batch_size: 16
  num_epochs: 3
  learning_rate: 5.0e-5
  warmup_ratio: 0.1
  max_length: 256
  save_path: "best_student_model"
